# Linux VM Provisioning with Docker and Ansible

### m0 Architecture
![Architecture Diagram](images/m0.png)

This project demonstrates provisioning a Linux virtual machine using **Docker** and configuring it with **Ansible**. The main goal is to **learn Ansible** in a simple and controlled environment.

---
## Milestone 0 â€“ Environment Provisioning
## Provisioning Approach

I was presented with three options to provision a Linux VM:

- On-premises
- Cloud-based
- Virtualized

I chose the **Virtualized** option because it offers more control and simplicity, which is ideal for this learning project.

### Virtualized Options

Within the virtualized approach, there are several methods:

| Feature | VirtualBox VM | Multipass VM | Docker Container |
|---------|---------------|--------------|-----------------|
| **Setup** | Medium | Very easy | Very easy |
| **OS Isolation** | Full VM | Full VM | Shared kernel |
| **Systemd / Services** | âœ… Full | âœ… Full | âŒ Limited |
| **SSH Access** | âœ… Yes | âœ… Yes | âš  Needs setup |
| **Networking** | Manual | Auto IP | Port mapping |
| **Resource Use** | High | Moderate | Very low |
| **Startup Time** | Slow | Fast | Very fast |
| **Best for Ansible** | âœ… Full OS-level | âœ… Full OS-level | âš  Limited |
| **Running Apps (Grafana/Prometheus)** | âœ… Yes | âœ… Yes | âœ… Perfect |
| **Multiple Nodes** | âœ… Heavy | âœ… Easy | âœ… Easy, lightweight |

I considered **Multipass VM** and **Docker Container**. Both are quick and resource-friendly.  
I leaned toward Docker because of familiarity and future milestones involving Grafana and Prometheus. However, the primary goal is to learn Ansible. Multipass offers **full OS-level access** and **realistic networking**, but I chose Docker for simplicity and compatibility with WSL.

---

## Setting up the VM with Docker Container

### Prerequisites

- WSL2 installed
- Docker installed

### Pull the Ubuntu LTS Image

```bash
docker pull ubuntu:24.04
```

### Create a Dockerfile

Instead of manually configuring a VM with Python, Git, Docker, and SSH, I created a Docker image with everything preconfigured:

```dockerfile
FROM ubuntu:24.04

# Avoid prompts during install
ENV DEBIAN_FRONTEND=noninteractive

# Update and install core tools
RUN apt update && \
    apt install -y \
        openssh-server \
        python3 python3-pip python3-apt \
        sudo git curl wget vim net-tools \
        apt-transport-https ca-certificates lsb-release gnupg && \
    mkdir /var/run/sshd && \
    echo 'root:root' | chpasswd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config

# Optional: Install Docker CLI (not full Docker Engine)
RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker.gpg && \
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker.gpg] \
    https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" > /etc/apt/sources.list.d/docker.list && \
    apt update && apt install -y docker-ce-cli

# Expose SSH
EXPOSE 22

# Start SSH daemon
CMD ["/usr/sbin/sshd", "-D"]
```

### Build the Docker Image

```bash
docker build -t ansible-node:latest .
```

### Create a Docker Compose File

`docker-compose.yml`:

```yaml
services:
  ansible-node:
    image: ansible-node:latest
    container_name: ansible-node
    ports:
      - "2222:22"
    tty: true
    restart: unless-stopped
```

### Start the Container

```bash
docker-compose up -d
```

Check the container status:

```bash
docker ps -a
```

---

## Connecting to the VM

### Connect via SSH

```bash
ssh root@127.0.0.1 -p 2222
# password: root
```

### Set up SSH Keys for Ansible

Generate a key on your local machine:

```bash
ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa
```

Copy the public key to the container:

```bash
ssh-copy-id -i ~/.ssh/id_rsa -p 2222 root@127.0.0.1
```

---

## Ansible Configuration

### Create Ansible Inventory

`inventory.ini`:

```ini
[local_nodes]
ansible-node ansible_host=127.0.0.1 ansible_port=2222 ansible_user=root ansible_ssh_private_key_file=~/.ssh/id_rsa
```

### Test Connectivity

```bash
ansible -i inventory.ini local_nodes -m ping
```

Expected output:

```json
ansible-node | SUCCESS => {"changed": false, "ping": "pong"}
```

---

## VS Code Remote Development

### Installing the Remote SSH Extension

To connect VS Code directly to the container via SSH, install the Remote - SSH extension:

1. Open VS Code
2. Go to Extensions (Ctrl+Shift+X)
3. Search for "Remote - SSH"
4. Install the extension by Microsoft

### Connecting to the Container

#### Method 1: WSL Connection

If you're working in WSL and want to connect to your WSL environment:

1. Look at the bottom-left corner of VS Code for the connection status indicator
2. Click on the status indicator (it shows your current connection type)
3. Select "Connect to WSL" from the dropdown menu

#### Method 2: SSH Connection to Container

To connect directly to the Docker container (or any other Machine):

1. Click on the connection status indicator in the bottom-left corner
2. Select "Connect to Host..." from the dropdown menu
3. Choose "127.0.0.1:2222" (or add it if not listed)
4. If you've set up SSH keys properly, you'll connect without a password
5. If not, you'll be prompted for the password (default: `root`)

Once connected, VS Code will open a new window running inside the container environment. This is particularly useful when you need to:
- Edit configuration files directly on the target system
- Set up and configure Grafana and Prometheus
- Debug applications running in the container
- Have a complete development environment within the provisioned VM

---

## Project Structure at Milestone 0

```
.
â”œâ”€â”€ docker-compose.yml    # Container orchestration
â”œâ”€â”€ Dockerfile           # VM image definition
â”œâ”€â”€ inventory.ini        # Ansible inventory
â””â”€â”€ readme.md           # This documentation
```

---
## Milestone 1 - Ansible Setup

### m1 Architecture
![Architecture Diagram](images/m1.png)

### Ansible Configuration

#### ansible.cfg

Created the Ansible configuration file to set up project paths and disable unnecessary warnings:

```ini
[defaults]
inventory = ./inventory/inventory.ini
roles_path = ./roles
host_key_checking = False
retry_files_enabled = False
deprecation_warnings = False
```

- `inventory`: Points to the inventory file location
- `roles_path`: Defines where to find Ansible roles
- `host_key_checking`: Disabled for development environment
- `retry_files_enabled`: Disabled to avoid clutter
- `deprecation_warnings`: Disabled for cleaner output

#### playbooks/site.yml

Created the main playbook that executes roles on defined hosts:

```yaml
---
- name: Configure local_nodes
  hosts: local_nodes
  become: yes
  roles:
    - moon-buggy
```

### Creating the moon-buggy Role

Following Ansible role best practices, I created a role to install the moon-buggy game:

#### defaults/main.yml

```yaml
---
package_name: moon-buggy
```

#### tasks/main.yml

```yaml
---
- name: Install moon-buggy game
  apt:
    name: "{{ package_name }}"
    state: present
    update_cache: yes
```

**Module Parameters Explained:**
- `apt`: Package manager module for Debian/Ubuntu
- `name`: Name of the package to install (using variable from defaults)
- `state: present`: Ensures the package is installed
- `update_cache: yes`: Runs `apt-get update` before installation

### Project Restructuring

Following Ansible best practices, I restructured the project to separate infrastructure and configuration management:

```
projeto/
â”œâ”€â”€ ansible-project/
â”‚   â”œâ”€â”€ ansible.cfg
â”‚   â”œâ”€â”€ inventory/
â”‚   â”‚   â””â”€â”€ inventory.ini
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”‚   â””â”€â”€ site.yml
â”‚   â””â”€â”€ roles/
â”‚       â””â”€â”€ moon-buggy/
â”‚           â”œâ”€â”€ defaults/
â”‚           â”‚   â””â”€â”€ main.yml
â”‚           â””â”€â”€ tasks/
â”‚               â””â”€â”€ main.yml
â”‚
â”œâ”€â”€ infra-project/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”‚
â””â”€â”€ readme.md
```

### Running the Playbook

From the `ansible-project` directory, execute the playbook:

```bash
ansible-playbook playbooks/site.yml
```

### Verification

SSH into the container and verify the package installation:

```bash
ssh root@127.0.0.1 -p 2222
dpkg -l | grep moon-buggy
```

The command should show that the moon-buggy package is successfully installed.

---

## Milestone 2 - Monitoring and Visualization

### m2 Architecture
![Architecture Diagram](images/m2.png)

### Prepare docker image with new services

In this new Milestone cronjob was needed to execute periodically metrics. The ubuntu docker images does not cronjob installed, so we need to pre-install it on the Dockerfile and also enable the service.

```dockerfile
FROM ubuntu:24.04

# Avoid prompts during install
ENV DEBIAN_FRONTEND=noninteractive

# Update and install core tools
RUN apt update && \
    apt install -y \
        openssh-server \
        python3 python3-pip python3-apt \
        sudo git curl wget vim net-tools \
        apt-transport-https ca-certificates lsb-release gnupg \
        cron && \
    mkdir /var/run/sshd && \
    echo 'root:root' | chpasswd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config

# Optional: Install Docker CLI (not the full Docker Engine)
RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker.gpg && \
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker.gpg] \
    https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" > /etc/apt/sources.list.d/docker.list && \
    apt update && apt install -y docker-ce-cli

RUN pip install --break-system-packages docker requests

# Create startup script to run both SSH and cron
RUN echo '#!/bin/bash\nservice cron start\n/usr/sbin/sshd -D' > /start.sh && \
    chmod +x /start.sh

# Expose SSH
EXPOSE 22

# Start both cron and SSH daemon
CMD ["/start.sh"]
```

After recreating we are ready to start the milestone

### Write a script to gather CPU, memory, disk, and uptime metrics

#### Overview

This milestone implements a comprehensive monitoring stack using Docker containers orchestrated by Ansible. The solution collects system metrics (CPU, memory, disk usage), stores them in Prometheus, and visualizes them through Grafana dashboards.

#### Architecture

The monitoring solution consists of:

- **Custom metrics collection script** running via cron jobs
- **Prometheus** for metrics storage and collection
- **Node Exporter** for system metrics exposure
- **Grafana** for data visualization and dashboards
- **Automated deployment** via Ansible playbooks

### Metrics Collection Implementation

#### System Metrics Script

Created `system-metrics.sh` to collect CPU, memory, and disk metrics:

```bash
#!/bin/bash
# Generate system metrics for Prometheus textfile collector

METRICS_DIR="/var/lib/node_exporter/textfile_collector"
METRICS_FILE="$METRICS_DIR/system_metrics.prom"

mkdir -p "$METRICS_DIR"

# CPU Usage (percentage)
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | awk -F'%' '{print 100-$1}')

# Memory Usage (percentage)
MEMORY_USAGE=$(free | grep Mem | awk '{printf "%.2f", ($3/$2) * 100.0}')

# Disk Usage (percentage) for root filesystem
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')

# Generate Prometheus metrics
cat > "$METRICS_FILE" << EOF
# HELP system_cpu_usage_percent System CPU usage percentage
# TYPE system_cpu_usage_percent gauge
system_cpu_usage_percent $CPU_USAGE

# HELP system_memory_usage_percent System memory usage percentage
# TYPE system_memory_usage_percent gauge
system_memory_usage_percent $MEMORY_USAGE

# HELP system_disk_usage_percent System disk usage percentage
# TYPE system_disk_usage_percent gauge
system_disk_usage_percent $DISK_USAGE
EOF
```

**Key Features:**

- Collects CPU usage using `top` command
- Calculates memory usage percentage from `/proc/meminfo` via `free`
- Monitors root filesystem disk usage with `df`
- Outputs metrics in Prometheus format compatible with textfile collector
- Creates metrics directory automatically if it doesn't exist

#### Sub-minute Collection Runner

Implemented `subminute_runner.sh` for frequent metric collection:

```bash
#!/bin/bash
# Run system metrics collection every 10 seconds for 1 minute

for i in {1..6}; do
    /opt/metrics/system-metrics.sh
    if [ $i -lt 6 ]; then
        sleep 10
    fi
done
```

This script runs the metrics collection 6 times with 10-second intervals, providing sub-minute granularity.

#### Cron Job Configuration

Configured automated execution via cron:

```bash
# Run every minute to collect metrics more frequently
* * * * * /opt/metrics/subminute_runner.sh
```

### Docker Infrastructure

#### Docker Compose Orchestration

Implemented 2 docker-compose to setup the monitoring stack, one for prometheus and node-exporter, and the other for grafana:

docker-compose (prometheus/node-exporter):

```yaml
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - metrics_data:/var/lib/node_exporter/textfile_collector
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.textfile.directory=/var/lib/node_exporter/textfile_collector'
    restart: unless-stopped
    networks:
      - monitoring

volumes:
  prometheus_data:
  metrics_data:

networks:
  monitoring:
    external: true
    name: monitoring
```

docker-compose (grafana):

```yaml
services:
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - ./provisioning:/etc/grafana/provisioning
      - ./dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    restart: unless-stopped
    networks:
      - monitoring

volumes:
  grafana_data:

networks:
  monitoring:
    external: true
    name: monitoring
```

**Infrastructure Highlights:**

- Persistent volumes for data retention
- Proper volume mounts for textfile collector integration
- Auto-restart policies for high availability
- Grafana provisioning for automated configuration

### Grafana Configuration

#### Datasource Provisioning

Automated Prometheus datasource configuration:

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    uid: prometheus
```

The "uid" configuration is very important, it make sure to connect to that specific datasource. At first I did not have this setting, and grafana was not finding the correct datasource.

### Prometheus Configuration

Created `prometheus.yml` with custom configuration:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node_exporter'
    static_configs:
      - targets: ['node_exporter:9100']
```

**Configuration Details:**

- 15-second scrape interval for real-time monitoring
- Monitors both Prometheus itself and Node Exporter
- Uses Docker service names for container communication

### Dashboard Implementation

Created comprehensive system metrics dashboard (`system-metrics-dashboard.json`) featuring:

**CPU Metrics Panel:**

- Real-time CPU usage percentage
- Time-series visualization
- Color-coded thresholds (green < 50%, yellow < 80%, red â‰¥ 80%)

**Memory Metrics Panel:**

- Memory usage percentage tracking
- Historical trend analysis
- Alert thresholds for high memory usage

**Disk Usage Panel:**

- Root filesystem utilization
- Storage capacity monitoring
- Critical usage warnings

**Dashboard Features:**

- Auto-refresh every 5 seconds
- Responsive design for different screen sizes
- Interactive time range selection
- Prometheus query integration

### Ansible Automation

#### Metrics Collector Role

Created comprehensive Ansible role (`metrics_collector`) for automated deployment:

- Copy scripts (system-metrics.sh and subminute_runner.sh) to the target host;
- Create log and metrics directory's to match docker-compose mounts;
- Add a cronjob to the system and run the subminute_runner.sh file;

#### Prometheus Role

Created comprehensive Ansible role (Prometheus) for automated deployment:

- Check if prometheus dir exists;
- Copy config files from prometheus;
- Create monitoring network;
- Start Prometheus and node-exporter container
- After Prometheus container is running copy the rest of the configs (This can be updated)

#### Grafana Role

Created comprehensive Ansible role (Grafana) for automated deployment:

- Create Grafana dir;
- Copy config files from Grafana;
- Copy provisioning files (dashboard and datasource)
- Create monitoring network;
- Start Grafana container;
- After Grafana container is runing copy the rest of the configs (This can be updated)

#### Monitoring Stack Deployment

Main playbook (`metrics.yml`) orchestrates the complete deployment:

```yaml
---
- name: Deploy monitoring infrastructure
  hosts: local_nodes
  become: yes
  roles:
    - metrics_collector
    - prometheus
    - grafana

- name: Verify deployment
  hosts: local_nodes
  become: yes
  tasks:
    - name: Check if metrics collection is working
      stat:
        path: /var/lib/node_exporter/textfile_collector/system_metrics.prom
      register: metrics_file

    - name: Display metrics file status
      debug:
        msg: "Metrics file exists: {{ metrics_file.stat.exists }}"
```

### Deployment and Verification

We can deploy the monitoring system by:

```bash
ansible-playbook playbooks/metrics.yml
```

#### Service Verification

All monitoring services are running correctly:

```bash
CONTAINER ID   IMAGE                       COMMAND                  STATUS
a1b2c3d4e5f6   grafana/grafana:latest     "/run.sh"               Up 2 hours
b2c3d4e5f6a1   prom/prometheus:latest     "/bin/prometheus --câ€¦"   Up 2 hours
c3d4e5f6a1b2   prom/node-exporter:latest  "/bin/node_exporter â€¦"   Up 2 hours
```

#### Access Points

The monitoring stack is accessible via:

- **Prometheus UI**: [http://localhost:9090](http://localhost:9090/)
- **Grafana Dashboard**: [http://localhost:3000](http://localhost:3000/) (admin/admin)
- **Node Exporter Metrics**: [http://localhost:9100](http://localhost:9100/)

### Project Structure at Milestone 2

```
OnboardingChallenge/
â”œâ”€â”€ ansible-project/
â”‚   â”œâ”€â”€ ansible.cfg
â”‚   â”œâ”€â”€ inventory/
â”‚   â”‚   â””â”€â”€ inventory.ini
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”‚   â”œâ”€â”€ metrics.yml          # Monitoring deployment playbook
â”‚   â”‚   â”œâ”€â”€ reset-metrics.yml    # Cleanup and reset playbook
â”‚   â”‚   â””â”€â”€ site.yml            # Main configuration playbook
â”‚   â””â”€â”€ roles/
â”‚       â”œâ”€â”€ grafana/            # Grafana configuration role
â”‚       â”‚   â”œâ”€â”€ files/
â”‚       â”‚   â”‚   â”œâ”€â”€ docker-compose.yml
â”‚       â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ system-metrics-dashboard.json
â”‚       â”‚   â”‚   â””â”€â”€ provisioning/
â”‚       â”‚   â”‚       â”œâ”€â”€ dashboards/
â”‚       â”‚   â”‚       â”‚   â”œâ”€â”€ dashboard.yml
â”‚       â”‚   â”‚       â”‚   â””â”€â”€ metrics-dashboard.json
â”‚       â”‚   â”‚       â””â”€â”€ datasources/
â”‚       â”‚   â”‚           â””â”€â”€ datasource.yml
â”‚       â”‚   â””â”€â”€ tasks/
â”‚       â”‚       â””â”€â”€ main.yml
â”‚       â”œâ”€â”€ metrics_collector/   # Custom metrics collection role
â”‚       â”‚   â”œâ”€â”€ files/
â”‚       â”‚   â”‚   â”œâ”€â”€ subminute_runner.sh
â”‚       â”‚   â”‚   â””â”€â”€ system-metrics.sh
â”‚       â”‚   â””â”€â”€ tasks/
â”‚       â”‚       â””â”€â”€ main.yml
â”‚       â”œâ”€â”€ moon-buggy/         # Game installation role
â”‚       â”‚   â”œâ”€â”€ defaults/
â”‚       â”‚   â”‚   â””â”€â”€ main.yml
â”‚       â”‚   â””â”€â”€ tasks/
â”‚       â”‚       â””â”€â”€ main.yml
â”‚       â””â”€â”€ prometheus/         # Prometheus configuration role
â”‚           â”œâ”€â”€ files/
â”‚           â”‚   â”œâ”€â”€ docker-compose.yml
â”‚           â”‚   â””â”€â”€ prometheus.yml
â”‚           â””â”€â”€ tasks/
â”‚               â””â”€â”€ main.yml
â”œâ”€â”€ infra-project/
â”‚   â”œâ”€â”€ Monitoring/            # Additional monitoring utilities
â”‚   â”‚   â”œâ”€â”€ gather_metrics/
â”‚   â”‚   â”‚   â””â”€â”€ system_metrics.sh
â”‚   â”‚   â””â”€â”€ grafana-prometheus_setup/
â”‚   â”œâ”€â”€ VM-Configuration/
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ README.md                  # This comprehensive documentation
```

---

## Milestone 3: Python API with Grafana Dashboard Integration

### m23Architecture
![Architecture Diagram](images/m3.png)

### Project Structure

```
ansible-project/
â”œâ”€â”€ playbooks/
â”‚   â””â”€â”€ api-dashboard.yml          # Main deployment playbook
â”œâ”€â”€ roles/
â”‚   â”œâ”€â”€ simple_metrics_api/        # API deployment role
â”‚   â”‚   â”œâ”€â”€ files/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”‚   â”‚   â”œâ”€â”€ requirements.txt   # Python dependencies
â”‚   â”‚   â”‚   â”œâ”€â”€ Dockerfile         # Container definition
â”‚   â”‚   â”‚   â””â”€â”€ docker-compose.yml # Service orchestration
â”‚   â”‚   â””â”€â”€ tasks/
â”‚   â”‚       â””â”€â”€ main.yml           # Deployment tasks
â”‚   â””â”€â”€ grafana_api/               # Grafana deployment role
â”‚       â”œâ”€â”€ files/
â”‚       â”‚   â”œâ”€â”€ docker-compose.yml # Grafana container config
â”‚       â”‚   â”œâ”€â”€ provisioning/
â”‚       â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ datasource.yml    # API datasource config
â”‚       â”‚   â”‚   â””â”€â”€ dashboards/
â”‚       â”‚   â”‚       â””â”€â”€ dashboard.yml     # Dashboard provider
â”‚       â”‚   â””â”€â”€ dashboards/
â”‚       â”‚       â””â”€â”€ time-metrics-dashboard.json # Dashboard definition
â”‚       â””â”€â”€ tasks/
â”‚           â””â”€â”€ main.yml           # Grafana deployment tasks
â””â”€â”€ inventory/
    â””â”€â”€ inventory.ini              # Ansible inventory
```

### Features Implemented

#### âœ… Python FastAPI Application

**File**: `roles/simple_metrics_api/files/main.py`

- **Framework**: FastAPI with Uvicorn ASGI server
- **Endpoints**:
  - `GET /metrics` - Authenticated time metrics endpoint
  - `GET /health` - Public health check endpoint
- **Response Format**:
  ```json
  {
    "timestamp_iso": "2025-12-04T15:30:45.123456",
    "timestamp_unix": 1764862245,
    "timestamp_readable": "2025-12-04 15:30:45",
    "authenticated_user": "grafana"
  }
  ```

#### âœ… Authentication Implementation (Advanced Feature)

**Authentication Method**: Bearer Token Authentication

- **Security**: HTTP Bearer token scheme
- **Valid Token**: `grafana-token`
- **Protected Endpoint**: `/metrics` requires authentication
- **Public Endpoint**: `/health` accessible without authentication

**Usage Example**:
```bash
# Authenticated request
curl -H "Authorization: Bearer grafana-token" http://localhost:8000/metrics

# Public endpoint
curl http://localhost:8000/health
```

#### âœ… Containerization

**Docker Configuration**:
- **Base Image**: python:3.9-slim
- **Port Exposure**: 8000
- **Dependencies**: FastAPI, uvicorn
- **Network**: Custom Docker network for inter-container communication

**Key Files**:
- `Dockerfile` - Container image definition
- `docker-compose.yml` - Service orchestration
- `requirements.txt` - Python dependencies

#### âœ… Ansible Automation

**Main Playbook**: `playbooks/api-dashboard.yml`

**Deployment Process**:
1. Docker installation verification
2. Custom network creation (`metrics-network`)
3. API container deployment
4. Grafana container deployment with provisioning
5. Service health checks
6. Deployment summary

**Roles**:
- **simple_metrics_api**: Deploys authenticated API
- **grafana_api**: Deploys Grafana with dashboard provisioning

#### âœ… Grafana Dashboard Integration

**Dashboard Features**:
- **Three Panels**: ISO timestamp, readable timestamp, Unix timestamp
- **Data Source**: marcusolsson-json-datasource plugin
- **Authentication**: Bearer token integration
- **Auto-refresh**: 5-second intervals
- **Network**: Docker host IP connection for virtualized environments

**JSON Datasource Plugin Importance**:
The `marcusolsson-json-datasource` plugin is essential for this implementation because:
- **API Integration**: Enables Grafana to directly consume RESTful JSON APIs
- **Real-time Data**: Fetches live data from the FastAPI endpoint without intermediate storage
- **Authentication Support**: Built-in support for Bearer token authentication
- **JSONPath/JSONata**: Flexible data extraction using JSONPath or JSONata expressions
- **No Database Required**: Eliminates the need for time-series databases like Prometheus for simple API data
- **Lightweight Solution**: Perfect for monitoring APIs that already expose JSON metrics

**Technical Implementation**:
- **Datasource URL**: `http://172.19.0.1:8000` (Docker host IP)
- **Authentication Header**: `Authorization: Bearer grafana-token`
- **JSONPath Selectors**: `$.timestamp_iso`, `$.timestamp_readable`, `$.timestamp_unix`

#### âœ… Dashboard Export and Provisioning

**Exported Dashboard**: `roles/grafana_api/files/dashboards/time-metrics-dashboard.json`

**Automated Provisioning**:
- **Dashboard Provider**: File-based provisioning
- **Datasource Configuration**: Automated with authentication
- **Plugin Installation**: JSON datasource plugin auto-installed

### Deployment Instructions

#### Prerequisites
- Docker installed on target system
- Ansible installed
- Network connectivity for container communication

#### Quick Deployment
```bash
# Navigate to project directory
cd ansible-project/

# Run deployment playbook
ansible-playbook -i inventory/inventory.ini playbooks/api-dashboard.yml
```

#### Verification
```bash
# Check API health
curl http://localhost:8000/health

# Test authenticated endpoint
curl -H "Authorization: Bearer grafana-token" http://localhost:8000/metrics

# Access Grafana dashboard
# URL: http://localhost:3000
# Login: admin / admin
```

### Network Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Docker Host                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   FastAPI Container â”‚  â”‚  Grafana Container  â”‚â”‚
â”‚  â”‚   Port: 8000        â”‚  â”‚  Port: 3000         â”‚â”‚
â”‚  â”‚   /metrics (auth)   â”‚â—„â”€â”‚  JSON Datasource    â”‚â”‚
â”‚  â”‚   /health (public)  â”‚  â”‚  Bearer Token Auth  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚              â–²                        â–²          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   API Calls â”‚              â”‚  Dashboard  â”‚
    â”‚ (Bruno/     â”‚              â”‚   Access    â”‚
    â”‚  curl)      â”‚              â”‚ (Browser)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Advanced Features Implemented

#### ðŸ” Bearer Token Authentication
- **Security Layer**: Protects sensitive metrics endpoint
- **Token Management**: Configurable token validation
- **Error Handling**: Proper HTTP 401 responses for unauthorized access
- **Grafana Integration**: Seamless authentication with datasource

#### ðŸ“Š Dashboard Automation
- **Zero-Configuration**: Dashboard appears automatically after deployment
- **Field Mapping**: Proper JSON field extraction and display
- **Real-time Updates**: Live data refresh every 5 seconds
- **Responsive Design**: Optimized panel layout

#### ðŸ‹ Container Orchestration
- **Shared Networking**: Custom Docker network for service communication
- **Health Monitoring**: Automated service readiness checks
- **Volume Persistence**: Grafana data persistence across restarts

### API Testing

#### Authentication Testing
```bash
# Valid token - should return data
curl -H "Authorization: Bearer grafana-token" \
     http://localhost:8000/metrics

# Invalid token - should return 401
curl -H "Authorization: Bearer invalid-token" \
     http://localhost:8000/metrics

# No token - should return 401  
curl http://localhost:8000/metrics
```

#### Postman Configuration
1. **Method**: GET
2. **URL**: `http://localhost:8000/metrics`
3. **Authorization**: Bearer Token
4. **Token**: `grafana-token`

### Troubleshooting

#### Common Issues

**1. Dashboard Shows "No Data"**
- Verify API container is running: `docker ps`
- Test API connectivity: `curl http://localhost:8000/health`
- Check Grafana logs: `docker logs grafana-api`

**2. Authentication Errors**
- Verify token in datasource configuration
- Test manual API call with bearer token
- Check API logs for authentication failures

**3. Network Connectivity Issues**
- Ensure Docker network exists: `docker network ls`
- Verify containers are on same network: `docker network inspect metrics-network`
- Use Docker host IP for datasource URL in virtualized environments

### Conclusion

This milestone successfully demonstrates a complete end-to-end monitoring solution with:
- **Secure API** with Bearer token authentication
- **Containerized architecture** for scalability
- **Infrastructure as Code** with Ansible automation  
- **Real-time visualization** with Grafana dashboards
- **Production-ready deployment** with proper networking and security

The implementation showcases modern DevOps practices including API security, container orchestration, and automated provisioning for a robust monitoring infrastructure.

---
